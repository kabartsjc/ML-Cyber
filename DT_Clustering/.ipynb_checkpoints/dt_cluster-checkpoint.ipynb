{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Evaluation of Clustering Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Basic Considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous example, we use KMeans to try to cauterize the NSL-KDD dataset into five different types of groups: benign (regular traffic), DoS, probe, U2R, and R2L. However, as you can see in the following picture, it does not work very well, where the clusters have more than one cyber group type.\n",
    "\n",
    "![Clustering Analysis Process](imgs/cluster_results.jpg)\n",
    "\n",
    "\n",
    "For example, in the red delimited area, you see that in cluster 3, you have more than one element from the cyber group. You have DoS, benign, and r2l. **How to analyze the data in this situation?** \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Clustering Analysis (Cross-Tabulation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-tabulation, also known as cross-tab or contingency table, is a statistical tool used for categorical data. Categorical data involves values that are mutually exclusive to each other.\n",
    "\n",
    "Understanding how cross-tabulation works requires that, first, we will re-apply to the clusterization process using K-Means. To perform this task, we divide it into three phases:\n",
    "- Load and Adjust the dataset;\n",
    "- Prepare and clean the dataset; and\n",
    "- Training and Predict values, using KMeans."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1. Load and Adjust the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading dataset...\n",
      "train shape: (125973, 43)\n",
      "test shape: (22544, 43)\n",
      "loading dictionary ...\n",
      "adjusting the dataset ...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_dataset(trainfile, testfile,\n",
    "                 header_names):\n",
    "    train_df = pd.read_csv(trainfile, names=header_names)\n",
    "    print('train shape:',train_df.shape)\n",
    "    \n",
    "    test_df = pd.read_csv(testfile, names=header_names)\n",
    "    print('test shape:',test_df.shape)\n",
    "    \n",
    "    return train_df, test_df\n",
    "\n",
    "header_names = ['duration', 'protocol_type', 'service', 'flag', 'src_bytes',\n",
    "                    'dst_bytes', 'land', 'wrong_fragment', 'urgent', 'hot', 'num_failed_logins',\n",
    "                    'logged_in', 'num_compromised', 'root_shell', 'su_attempted', 'num_root', 'num_file_creations',\n",
    "                    'num_shells', 'num_access_files', 'num_outbound_cmds', 'is_host_login', 'is_guest_login',\n",
    "                    'count', 'srv_count', 'serror_rate', 'srv_serror_rate', 'rerror_rate', 'srv_rerror_rate',\n",
    "                    'same_srv_rate', 'diff_srv_rate', 'srv_diff_host_rate', 'dst_host_count', 'dst_host_srv_count',\n",
    "                    'dst_host_same_srv_rate', 'dst_host_diff_srv_rate', 'dst_host_same_src_port_rate',\n",
    "                    'dst_host_srv_diff_host_rate',\n",
    "                    'dst_host_serror_rate', 'dst_host_srv_serror_rate', 'dst_host_rerror_rate',\n",
    "                    'dst_host_srv_rerror_rate',\n",
    "                    'attack_type', 'success_pred']\n",
    "\n",
    "train_file_name = 'resources/ndd/KDDTrain+.txt'\n",
    "test_file_name='resources/ndd/KDDTest+.txt'\n",
    "\n",
    "print('loading dataset...')\n",
    "train_df, test_df= load_dataset(train_file_name, test_file_name,header_names)\n",
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "def load_dictionary(dictionary_name):\n",
    "    category = defaultdict(list)\n",
    "    category['benign'].append('normal')\n",
    "\n",
    "    with open(dictionary_name, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            attack, cat = line.strip().split(' ')\n",
    "            category[cat].append(attack)\n",
    "\n",
    "    mapping_dict = dict((v, k)\n",
    "                        for k in category for v in category[k])\n",
    "    return mapping_dict\n",
    "\n",
    "print('loading dictionary ...')\n",
    "dict_file = 'resources/ndd/training_attack_types.txt'\n",
    "dicta = load_dictionary(dict_file)\n",
    "\n",
    "def adjust_datasets(train_ds, test_ds, dicta, att_type_lb, catg_attk_lab,drop_list_label):\n",
    "    train_ds[catg_attk_lab] = train_ds[att_type_lb].map(lambda x: dicta[x])\n",
    "    test_ds[catg_attk_lab] = test_ds[att_type_lb].map(lambda x: dicta[x])\n",
    "\n",
    "    for i in range(0, len(drop_list_label)):\n",
    "        train_ds.drop([drop_list_label[i]], axis=1, inplace=True)\n",
    "        test_ds.drop([drop_list_label[i]], axis=1, inplace=True)\n",
    "      \n",
    "        \n",
    "import numpy as np\n",
    "def getColunssByClass(header_names):\n",
    "    nominal_idx = [1, 2, 3]\n",
    "    bin_idx = [6, 11, 13, 14, 20, 21]\n",
    "    numeric_idx = list(set(range(41)).difference(nominal_idx).difference(bin_idx))\n",
    "    col_names = np.array(header_names)\n",
    "    nominal_cols = col_names[nominal_idx].tolist()\n",
    "    binary_cols = col_names[bin_idx].tolist()\n",
    "    numeric_cols = col_names[numeric_idx].tolist()\n",
    "    return nominal_cols, binary_cols, numeric_cols\n",
    "\n",
    "\n",
    "print('adjusting the dataset ...')\n",
    "nominal_cols, binary_cols, numeric_cols = getColunssByClass(header_names)\n",
    "drop_list_label=[]\n",
    "drop_list_label.append('success_pred')\n",
    "adjust_datasets(train_df,test_df,dicta,'attack_type', 'attack_category',drop_list_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2. Prepare and clean the dataset\n",
    "\n",
    "Until this moment, the process is the same as that used in the 'cyber_cluster'Junpyer notebook. Following the previous task, it is required to cleaning and split the labels and data from the test and train dataset. \n",
    "\n",
    "The labels contain these values in the cited notebook: benign (regular traffic), DoS, probe, U2R, and R2L. However, as the clusters are not so accurate, the new idea is to have only two values (attack and normal). It is sufficient because the final goal is to identify attacks and not classify attacks by type.\n",
    "\n",
    "The main difference between the previous **prepare_data' method** and the current implementation, it is that we create a new label \"transaction_class\", that informs that package is an attack or benign (regular traffic)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaning data ...\n",
      "preparing data ...\n",
      "scalling data ...\n",
      "encoding data ...\n"
     ]
    }
   ],
   "source": [
    "def cleaning_data (train_df, test_df, num_cols):\n",
    "    train_df['su_attempted'].replace(2, 0, inplace=True)\n",
    "    test_df['su_attempted'].replace(2, 0, inplace=True)\n",
    "    train_df.drop('num_outbound_cmds', axis=1, inplace=True)\n",
    "    test_df.drop('num_outbound_cmds', axis=1, inplace=True)\n",
    "    num_cols.remove('num_outbound_cmds')\n",
    "\n",
    "print('cleaning data ...')\n",
    "cleaning_data(train_df,test_df,numeric_cols)\n",
    "\n",
    "def prepare_dataset(train_df, test_df, target_name,new_target_name, nominal_cols):\n",
    "\n",
    "    #split the dataset into target_labels and values\n",
    "    train_label = train_df[new_target_name]\n",
    "    train_x_raw = train_df.drop([new_target_name, target_name], axis=1)\n",
    "\n",
    "    test_label = test_df[new_target_name]\n",
    "    test_x_raw = test_df.drop([new_target_name, target_name], axis=1)\n",
    "\n",
    "    #convert the categorical values to numeric values\n",
    "    combined_df_raw = pd.concat([train_x_raw, test_x_raw])\n",
    "    combined_df = pd.get_dummies(combined_df_raw, columns=nominal_cols, drop_first=True)\n",
    "\n",
    "    train_x = combined_df[:len(train_x_raw)]\n",
    "    test_x = combined_df[len(train_x_raw):]\n",
    "    \n",
    "    ################################ main change, when comparec with the previous code ###############################\n",
    "    train_Y = train_label.apply(lambda x: 'normal' if x == 'benign' else 'attack')\n",
    "    train_df['transaction_class'] = train_label.apply(lambda x: 'normal' if x == 'benign' else 'attack')\n",
    "    \n",
    "    test_Y = test_label.apply(lambda x: 'normal' if x == 'benign' else 'attack')\n",
    "    test_df['transaction_class'] = test_label.apply(lambda x: 'normal' if x == 'benign' else 'attack')\n",
    "   \n",
    "    return train_x,train_label, test_x, test_label, train_Y, test_Y\n",
    "\n",
    "print('preparing data ...')\n",
    "train_x,train_label, test_x, test_label, train_Y, test_Y = prepare_dataset(train_df, test_df, 'attack_type','attack_category', nominal_cols)\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "def scalling(train_x, test_x, num_cols):\n",
    "    standard_scaler = StandardScaler().fit(train_x[num_cols])\n",
    "    train_x[num_cols] = standard_scaler.transform(train_x[num_cols])\n",
    "    test_x[num_cols] = standard_scaler.transform(test_x[num_cols])\n",
    "    \n",
    "    return train_x,  test_x\n",
    "\n",
    "print('scalling data ...')\n",
    "train_x,  test_x = scalling(train_x, test_x,numeric_cols)\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "def enconding_label(train_label, test_label):\n",
    "    # encoder the labels and transform in numeric values\n",
    "    # it is impoirtant to analyze the result\n",
    "    label_encoder = LabelEncoder()\n",
    "    train_label_np = train_label.to_numpy()\n",
    "    train_label_encod = label_encoder.fit_transform(train_label_np)\n",
    "\n",
    "    test_label_np = test_label.to_numpy()\n",
    "    test_label_encod = label_encoder.fit_transform(test_label_np)\n",
    "\n",
    "    return train_label_encod, test_label_encod, label_encoder\n",
    "\n",
    "print('encoding data ...')\n",
    "train_label_encod, test_label_encod, label_encoder = enconding_label(train_label,test_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3. Training and Predict values, using KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runing K-Means....\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def trainning (train_x, cluster_number):\n",
    "    kmeans = KMeans(n_clusters=cluster_number, random_state=17).fit(train_x)\n",
    "    return kmeans\n",
    "\n",
    "number_of_cluster = 5\n",
    "print('runing K-Means....')\n",
    "kmeans = trainning(train_x,number_of_cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4. Cross-Tabulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the model is trained, the next step is the generation of cross-tabulation. However, to perform this task, it is required to insert a new label in the train and test dataset with the result of the cluster (*'kmeans_y'*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting the new labels ....\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>transaction_class</th>\n",
       "      <th>attack</th>\n",
       "      <th>normal</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kmeans_y</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>485</td>\n",
       "      <td>526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>543</td>\n",
       "      <td>958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2036</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4643</td>\n",
       "      <td>8136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5126</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "transaction_class  attack  normal\n",
       "kmeans_y                         \n",
       "0                     485     526\n",
       "1                     543     958\n",
       "2                    2036       5\n",
       "3                    4643    8136\n",
       "4                    5126      86"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('setting the new labels ....')\n",
    "train_df['kmeans_y'] = kmeans_train_y\n",
    "\n",
    "kmeans_test_y = kmeans.predict(test_x)\n",
    "test_df['kmeans_y'] = kmeans_test_y\n",
    "\n",
    "pd.crosstab(test_df.kmeans_y, test_df.transaction_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Improve the Cluster Results using Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyzing the result, we can see that ***cluster 2 is an attack cluster***; but you cannot assume this premise for the others. The idea is to classify each cluster's results using a Decision Tree.\n",
    "\n",
    "First, we will create a decision tree engine and train that using the specific cluster results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defining the decision tree classifier...\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "def train_decision_tree(train_x, train_Y):\n",
    "    classifier = DecisionTreeClassifier(random_state=0)\n",
    "    classifier.fit(train_x, train_Y)\n",
    "    \n",
    "    return classifier\n",
    "\n",
    "print('defining the decision tree classifier...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Calculation to Cluster 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instantiating and fiting the decision tree to cluster 0\n",
      "prediction values to cluster 0\n",
      "confusion matrix to cluster 0\n",
      "[[309 176]\n",
      " [ 14 512]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, zero_one_loss\n",
    "\n",
    "train_y0 = train_df[train_df.kmeans_y==0]\n",
    "train_y0=train_y0.drop(['kmeans_y'], axis=1)\n",
    "\n",
    "test_y0 = test_df[test_df.kmeans_y==0]\n",
    "test_y0=test_y0.drop(['kmeans_y'], axis=1)\n",
    "\n",
    "print ('instantiating and fiting the decision tree to cluster 0')\n",
    "dtc0 = train_decision_tree(train_y0[numeric_cols],train_y0['transaction_class'])\n",
    "\n",
    "print ('prediction values to cluster 0')\n",
    "dtc0_pred_y = dtc0.predict(test_y0[numeric_cols])\n",
    "\n",
    "print ('confusion matrix to cluster 0')\n",
    "conf_mat = confusion_matrix(test_y0['transaction_class'], dtc0_pred_y)\n",
    "print(conf_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Calculation to Cluster 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instantiating and fiting the decision tree to cluster 1\n",
      "prediction values to cluster 1\n",
      "confusion matrix to cluster 0\n",
      "[[543   0]\n",
      " [  4 954]]\n"
     ]
    }
   ],
   "source": [
    "train_y1 = train_df[train_df.kmeans_y==1]\n",
    "train_y1=train_y1.drop(['kmeans_y'], axis=1)\n",
    "\n",
    "test_y1 = test_df[test_df.kmeans_y==1]\n",
    "test_y1=test_y1.drop(['kmeans_y'], axis=1)\n",
    "\n",
    "print ('instantiating and fiting the decision tree to cluster 1')\n",
    "dtc1 = train_decision_tree(train_y1[numeric_cols],train_y1['transaction_class'])\n",
    "\n",
    "print ('prediction values to cluster 1')\n",
    "dtc1_pred_y = dtc1.predict(test_y1[numeric_cols])\n",
    "\n",
    "print ('confusion matrix to cluster 0')\n",
    "conf_mat = confusion_matrix(test_y1['transaction_class'], dtc1_pred_y)\n",
    "print(conf_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Calculation to Cluster 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instantiating and fiting the decision tree to cluster 3\n",
      "prediction values to cluster 3\n",
      "confusion matrix to cluster 3\n",
      "[[1496 3147]\n",
      " [ 193 7943]]\n"
     ]
    }
   ],
   "source": [
    "train_y3 = train_df[train_df.kmeans_y==3]\n",
    "train_y3=train_y3.drop(['kmeans_y'], axis=1)\n",
    "\n",
    "test_y3 = test_df[test_df.kmeans_y==3]\n",
    "test_y3=test_y3.drop(['kmeans_y'], axis=1)\n",
    "\n",
    "print ('instantiating and fiting the decision tree to cluster 3')\n",
    "dtc3 = train_decision_tree(train_y3[numeric_cols],train_y3['transaction_class'])\n",
    "\n",
    "print ('prediction values to cluster 3')\n",
    "dtc3_pred_y = dtc3.predict(test_y3[numeric_cols])\n",
    "\n",
    "print ('confusion matrix to cluster 3')\n",
    "conf_mat = confusion_matrix(test_y3['transaction_class'], dtc3_pred_y)\n",
    "print(conf_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 Calculation to Cluster 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y4 = train_df[train_df.kmeans_y==4]\n",
    "train_y4=train_y4.drop(['kmeans_y'], axis=1)\n",
    "\n",
    "test_y4 = test_df[test_df.kmeans_y==4]\n",
    "test_y4=test_y4.drop(['kmeans_y'], axis=1)\n",
    "\n",
    "print ('instantiating and fiting the decision tree to cluster 4')\n",
    "dtc4 = train_decision_tree(train_y4[numeric_cols],train_y4['transaction_class'])\n",
    "\n",
    "print ('prediction values to cluster 4')\n",
    "dtc4_pred_y = dtc4.predict(test_y4[numeric_cols])\n",
    "\n",
    "print ('confusion matrix to cluster 4')\n",
    "conf_mat = confusion_matrix(test_y4['transaction_class'], dtc4_pred_y)\n",
    "print(conf_mat)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
